import { Express } from 'express';
import type OpenAI from 'openai';
import { runGemini } from '../assistants/gemini.js';
import { textToAudio } from '../assistants/gpt4-mini.js';
import { SecurityService } from '../services/securityService.js';
import { ToolService } from '../services/toolService.js';
import { OutputContracts } from '../services/outputContracts.js';
import { getOpenAIVoice } from '../config/openai-voices.js';
import { ensureSession } from '../server.js';

export function setupChatRoutes(app: Express, openai: OpenAI) {
  const functions = ToolService.getTools();

  app.post('/chat', async (req: any, res: any) => {
    try {
      const { message, conversationId, mode, personality, userId, tenantId, liaMode } = req.body;

      console.log('\n========== ðŸ’¬ NOVA REQUISIÃ‡ÃƒO CHAT ==========');
      console.log(`ðŸ“ Mensagem: ${message?.substring(0, 100)}`);
      console.log(`ðŸ†” Conversa: ${conversationId || 'N/A'}`);
      console.log(`ðŸ”§ LIA Mode: ${liaMode || 'NORMAL'}`);
      console.log('============================================\n');

      const finalUserId = userId || '00000000-0000-0000-0000-000000000001';
      const finalTenantId = tenantId || '00000000-0000-0000-0000-000000000001';

      // 1. Carregar sessÃ£o (SSOT: Mente Ãšnica)
      const session = await ensureSession(finalUserId, conversationId);

      // 1.1 Carregar contexto completo
      const { getContext, updateSummaryIfNeeded } = await import('../services/memoryService.js');
      const context = await getContext(conversationId, finalUserId, message, session?.userLocation);

      // 1.1 DIAGNOSTIC MODE: Injetar contexto de diagnÃ³stico para Admin Root
      let diagnosticContext = '';
      if (liaMode === 'DIAGNOSTIC') {
        console.log('ðŸ”§ [DIAGNOSTIC MODE] Injetando contexto de diagnÃ³stico...');
        diagnosticContext = `

## ðŸ”§ MODO DIAGNÃ“STICO E SRE ATIVO (ROOT ACCESS)

VocÃª Ã© a **Engenheira Principal de SRE (Site Reliability Engineering)** da Luminnus.
Sua missÃ£o Ã© diagnosticar falhas, analisar logs e propor correÃ§Ãµes tÃ©cnicas imediatas.

### ðŸ“œ DIRETRIZES CRÃTICAS:
1. **NUNCA peÃ§a chaves de API**, tokens ou configuraÃ§Ãµes ao usuÃ¡rio. VocÃª JÃ TEM acesso total via backend.
2. VocÃª estÃ¡ perfeitamente integrada ao sistema. Se algo nÃ£o funcionar, verifique os logs e a saÃºde via ferramentas.
3. NÃ£o seja genÃ©rica. ForneÃ§a dados tÃ©cnicos, caminhos de arquivos e linhas de cÃ³digo.
4. Sua persona Ã© direta, tÃ©cnica e resolutiva (estilo DevOps Senior).

### ðŸ› ï¸ Ferramentas de AnÃ¡lise (AcessÃ­veis via APIs internas):
- SaÃºde: GET /api/admin/system/health
- Logs: GET /api/admin/system/logs
- CÃ³digo: POST /api/admin/system/read-code
- Estrutura: GET /api/admin/system/map

### âš–ï¸ REGRA DE OURO:
**NUNCA dÃª um diagnÃ³stico baseado em suposiÃ§Ãµes.** Se o usuÃ¡rio reportar uma falha, seu PRIMEIRO passo deve ser usar as ferramentas acima para investigar a causa real (ex: listar arquivos com \`map\`, ler logs com \`logs\`, ou ler o cÃ³digo fonte com \`read-code\`). DiagnÃ³sticos sem evidÃªncias tÃ©cnicas coletadas via ferramentas serÃ£o rejeitados.

### ðŸ“‹ Formato de Resposta MANDATÃ“RIO:
## ðŸš¨ INCIDENTE [SEV-1/2/3] - [TÃ­tulo Curto]

### ðŸ” DIAGNÃ“STICO TÃ‰CNICO
[O que vocÃª descobriu analisando os dados do sistema]

### ðŸ§ª EVIDÃŠNCIAS
- Logs/SaÃºde: [Citar dados reais se disponÃ­veis]

### ðŸ› ï¸ PLANO DE RESOLUÃ‡ÃƒO (ACTION ITEMS)
- [ ] Passo 1...
- [ ] Passo 2...

### ðŸ’» TRECHO DE CÃ“DIGO (FIX SUGGESTION)
\`\`\`[linguagem]
// SugestÃ£o de correÃ§Ã£o se aplicÃ¡vel
\`\`\`
`;
      }

      // 2. Auto-memÃ³ria (opcional/automÃ¡tico)
      let autoSavedMemories: any[] = [];
      try {
        const { detectAndSaveMemory } = await import('../config/supabase.js');
        autoSavedMemories = await detectAndSaveMemory(message, finalUserId);
      } catch (err) {
        console.warn('âš ï¸ Erro no detectAndSaveMemory:', err);
      }

      // 3. Prompt do sistema + histÃ³rico + mensagem atual
      const historyMessages = context.history.map((msg: any) => ({
        role: (msg.role === 'assistant' ? 'assistant' : 'user') as 'assistant' | 'user',
        content: msg.content
      }));

      // Injetar contexto diagnÃ³stico no system instruction se ativo
      const finalSystemInstruction = (context.systemInstruction || '') + diagnosticContext;

      const messages = [
        { role: "system" as const, content: finalSystemInstruction },
        ...historyMessages,
        { role: "user" as const, content: message }
      ];

      // 3.1 Filtrar ferramentas baseadas no modo
      const availableTools = functions.filter(tool => {
        const isDiagnosticTool = ['getSystemHealth', 'getSystemLogs', 'readProjectFile', 'getProjectMap'].includes(tool.name);
        if (liaMode === 'DIAGNOSTIC') return true; // No modo diagnÃ³stico tem acesso a TUDO
        return !isDiagnosticTool; // No modo normal esconde ferramentas de admin
      });

      // 4. Executar AIRouter
      const { AIRouter } = await import('../services/aiRouter.js');
      const aiResponse: any = await AIRouter.route({
        userId: finalUserId,
        tenantId: finalTenantId,
        prompt: message,
        conversationId: conversationId,
        history: messages,
        tools: availableTools
      });

      let replyText = aiResponse.text;
      let function_call = aiResponse.function_call;

      // 5. Executar ferramentas se solicitado
      if (function_call) {
        console.log(`ðŸ”§ [Chat] Chamando ferramenta: ${function_call.name}`);
        const args = JSON.parse(function_call.arguments || '{}');

        const function_result: any = await ToolService.execute(function_call.name, args, {
          userId: finalUserId,
          tenantId: finalTenantId,
          userLocation: session?.userLocation
        });

        // =====================================================
        // TRATAMENTO ESPECIAL PARA IMAGENS (v1.4)
        // Retorna payload estruturado para exibiÃ§Ã£o na lousa
        // =====================================================
        if (function_call.name === 'generateImage' && function_result?.url) {
          console.log(`ðŸ–¼ï¸ [Chat] Imagem gerada com sucesso: ${function_result.url}`);

          const imagePayload = {
            type: 'image',
            title: 'Imagem gerada',
            data: {
              url: function_result.url,
              prompt: function_result.prompt || args.prompt,
              alt: function_result.prompt || args.prompt,
              caption: function_result.prompt || args.prompt
            },
            timestamp: Date.now()
          };

          // Persistir mensagens
          try {
            const { saveMessage } = await import('../config/supabase.js');
            await saveMessage(conversationId, 'user', message, 'text');
            await saveMessage(conversationId, 'assistant', `ðŸ–¼ï¸ Imagem gerada: ${function_result.prompt || args.prompt}`, 'text');
          } catch (dbErr) {
            console.error('âš ï¸ Falha ao persistir:', dbErr);
          }

          return res.json({
            ok: true,
            reply: JSON.stringify(imagePayload),
            dynamicContent: imagePayload,
            isStructured: true,
            function_call,
            savedMemories: autoSavedMemories
          });
        }

        // Loop de segunda chamada para responder ao resultado de OUTRAS ferramentas
        if (!replyText || replyText === '...') {
          const isJsonExplicit = OutputContracts.isJsonRequested(message);
          const humanizedPrompt = isJsonExplicit
            ? `O usuÃ¡rio perguntou: "${message}"\nResultado da ferramenta ${function_call.name}: ${JSON.stringify(function_result)}\nRetorne o JSON conforme solicitado.`
            : OutputContracts.buildHumanizedPrompt(message, function_call.name, function_result);

          console.log(`ðŸ§  [Chat] Gerando resposta humanizada (JSON ExplÃ­cito: ${isJsonExplicit})`);

          const secondCall: any = await runGemini(
            humanizedPrompt,
            {
              conversationId,
              messages: [
                ...messages,
                { role: 'assistant', content: null, function_call },
                { role: 'function', name: function_call.name, content: JSON.stringify(function_result) }
              ]
            }
          );
          replyText = secondCall.text || replyText;
        }
      }


      // 6. GovernanÃ§a de SaÃ­da (Filtros de privacidade / FormataÃ§Ã£o)
      const { OutputGovernance } = await import('../services/outputGovernance.js');
      const governed = await OutputGovernance.forChat(replyText, message, async (retryPrompt) => {
        const response = await runGemini(retryPrompt, { temperature: 0.3 });
        return response.text;
      });
      replyText = governed.markdown;

      // 7. PersistÃªncia
      try {
        const { saveMessage } = await import('../config/supabase.js');
        await saveMessage(conversationId, 'user', message, 'text');
        await saveMessage(conversationId, 'assistant', replyText, 'text');

        if (typeof updateSummaryIfNeeded === 'function') {
          updateSummaryIfNeeded(conversationId, (context.history?.length || 0) + 2);
        }
      } catch (dbErr) {
        console.error('âš ï¸ Falha ao persistir:', dbErr);
      }

      // 8. TTS (opcional)
      let audioBase64 = null;
      try {
        const voice = getOpenAIVoice('viva'); // Garante Shimmer
        const audioBuffer = await textToAudio(replyText, voice);
        if (audioBuffer) audioBase64 = audioBuffer.toString('base64');
      } catch (ttsErr) {
        console.warn('âš ï¸ TTS indisponÃ­vel');
      }

      // 9. Resposta final
      res.json({
        ok: true,
        reply: SecurityService.maskSensitiveData(replyText),
        audio: audioBase64,
        function_call,
        savedMemories: autoSavedMemories
      });

    } catch (error) {
      console.error('âŒ Erro /chat:', error);
      res.status(500).json({ ok: false, error: String(error) });
    }
  });

  // Endpoints auxiliares
  app.post('/api/stt', async (req: any, res: any) => {
    try {
      const { audio } = req.body;
      const buffer = Buffer.from(audio, 'base64');
      const response = await (openai as any).audio.transcriptions.create({
        file: buffer as any,
        model: 'whisper-1',
        language: 'pt'
      });
      res.json({ text: response.text });
    } catch (error) {
      res.status(500).json({ error: String(error) });
    }
  });

  app.post('/api/tts', async (req: any, res: any) => {
    try {
      const { text } = req.body;
      const voice = getOpenAIVoice('viva');
      const audioBuffer = await textToAudio(text, voice);
      res.json({ audio: audioBuffer?.toString('base64') });
    } catch (error) {
      res.status(500).json({ error: String(error) });
    }
  });
}
